# The Governed AI Systems Doctrine

This is not a framework for building tools.

This is a framework for **controlling consequences**.

---

## The Core Claim

Most AI and automation failures do not come from bad models.

They come from:
- Missing ownership
- Missing stop authority
- Missing reversibility
- Missing legibility
- Missing incentive alignment

In other words:

> Execution outruns governance.

---

## The Prime Directive

No system is allowed to act beyond the boundary of human authority.

Capability does not grant permission.

Scale does not grant legitimacy.

Speed does not grant sovereignty.

---

## The Control Principle

Every meaningful system must have:

- A human who owns the outcome
- A human who can stop it
- A human who can reverse it
- A human who can explain it
- A human who accepts the incentives

If any of these are missing, the system is **not governed**.

---

## The Catastrophe Pattern

All major system failures share the same shape:

- Local optimization
- Distributed responsibility
- Hidden irreversibility
- Authority without accountability
- Speed without brakes

---

## The Design Law

> Any system that cannot be cleanly stopped, understood, and reversed will eventually cause harm.

This is not a moral claim.

It is an engineering claim.

---

## The Purpose of This Repository

This repository exists to:

- Define the non-negotiable laws of governed systems
- Provide diagnostics to detect unsafe designs
- Provide architectures that enforce control
- Provide case studies of what happens when this is ignored
