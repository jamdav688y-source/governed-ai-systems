# The 5 Core Diagnostic Questions

Every complex AI, automation, or decision system that fails does so because it fails one or more of these questions.

These are not technical questions.
They are control questions.

---

## 1. Who owns the outcome?

If this system causes harm, loss, or failure:
- Is there a single accountable human or role?
- Or is responsibility diffused across components, teams, or vendors?

If no one owns the outcome, the system is already unsafe.

---

## 2. Who can stop it?

If this system begins behaving incorrectly:
- Who can halt it?
- How fast?
- Under what conditions?
- By what authority?

If stopping requires heroics, it is not governed.

---

## 3. Can we reverse it?

If this system makes a bad decision:
- Can we undo the effects?
- Can we restore the prior state?
- Or does it create irreversible consequences?

Irreversibility without explicit consent is a design failure.

---

## 4. Can we explain what it is doing?

When this system acts:
- Can a human explain why?
- Can they trace the decision path?
- Can they defend it to another human?

If not, you do not control it â€” you merely observe it.

---

## 5. What incentives does it obey?

- What does this system optimize for?
- Who benefits when it succeeds?
- Who pays when it fails?

Misaligned incentives always defeat good intentions.

---

# Interpretation

A system that fails any one of these is a **risk**.

A system that fails two or more is a **liability**.

A system that fails three or more is **guaranteed to eventually cause harm**.
