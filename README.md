# Governed AI Systems

> Intelligence is not the risk.  
> Speed without control is.

This repository is a living reference architecture for **building governable AI, automation, and decision systems**.

Most AI failures do not happen because models are weak.  
They happen because **execution outruns governance**.

Modern AI systems are not tools.  
They are **decision factories**.

And every decision factory must answer five questions:

## The Governance Test

1. What decision is actually being made?
2. Who owns the outcome of that decision?
3. What happens if this is wrong?
4. Who can stop it, and under what conditions?
5. How do we recover?

If any of these are unclear, implicit, or political:
You do not have a system.
You have a **liability generator**.

---

## Core Doctrine

A system is not controlled by how intelligent it is.  
A system is controlled by what it is **not allowed** to do.

If you cannot:
- See what it is doing and why
- Stop it while it is running
- Reverse what it just did
- Name who owns the outcome
- Repair the damage

Then you do not control it.  
You are only **accelerating it**.

---

## What This Repository Is

This is not:
- A prompt collection
- A framework dump
- A tool showcase

This is:
- A **discipline**
- A **control-plane philosophy**
- A **failure-mode-first architecture library**

---

## Focus Areas

- Decision ownership
- Stop-authority and escalation
- Reversibility and rollback
- Auditability and legibility
- Blast-radius containment
- Human-in-the-loop governance
- Failure recovery patterns

---

## The Prime Question

> What happens when this is wrong?

If a system cannot answer that clearly, it is not ready to scale.

---

## Status

This repository is under active construction.  
It is intended to become a public reference for:

- AI governance
- Agentic system control planes
- High-stakes automation
- Decision system safety
- Organizational risk architecture

---

> Speed is easy.  
> Control is the real product.
Add core doctrine README
